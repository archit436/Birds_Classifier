{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from torch.utils.data import TensorDataset, Subset, Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Loading Individual Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_dataset(test_dataset, target_samples_per_class=500):\n",
    "    # Gather labels from the test dataset.\n",
    "    test_labels = []\n",
    "    for i in range(len(test_dataset)):\n",
    "        _, label = test_dataset[i]  \n",
    "        # Extract the label from the dataset item.\n",
    "        test_labels.append(label.item())\n",
    "\n",
    "    # Count samples per class in the test dataset\n",
    "    class_counts = Counter(test_labels)\n",
    "    print(f\"Original class distribution: {dict(class_counts)}\")\n",
    "\n",
    "    # Identify classes that need oversampling\n",
    "    classes_to_oversample = {\n",
    "        cls: (target_samples_per_class - count)\n",
    "        for cls, count in class_counts.items()\n",
    "        if count < target_samples_per_class\n",
    "    }\n",
    "\n",
    "    # If no class is under the target, simply return the original dataset\n",
    "    if not classes_to_oversample:\n",
    "        print(\"No oversampling needed - all classes have enough samples.\")\n",
    "        return test_dataset\n",
    "\n",
    "    # Map each class to the list of indices that contain that class\n",
    "    class_indices_map = {cls: [] for cls in class_counts.keys()}\n",
    "    for i, lbl in enumerate(test_labels):\n",
    "        class_indices_map[lbl].append(i)\n",
    "\n",
    "    # Generate the new indices by oversampling\n",
    "    additional_indices = []\n",
    "    for cls, num_needed in classes_to_oversample.items():\n",
    "        # Randomly sample (with replacement) from the available indices of this class\n",
    "        oversampled = np.random.choice(class_indices_map[cls], size=num_needed, replace=True)\n",
    "        additional_indices.extend(oversampled)\n",
    "\n",
    "    # Combine original indices with the newly oversampled ones\n",
    "    all_indices = list(range(len(test_dataset))) + additional_indices\n",
    "\n",
    "    # Create a new Subset using these indices\n",
    "    balanced_dataset = Subset(test_dataset, all_indices)\n",
    "\n",
    "    # Optional: verify the new distribution\n",
    "    balanced_labels = []\n",
    "    for idx in all_indices:\n",
    "        _, label = test_dataset[idx]\n",
    "        balanced_labels.append(int(label))\n",
    "    balanced_counts = Counter(balanced_labels)\n",
    "    print(f\"Balanced class distribution: {dict(balanced_counts)}\")\n",
    "\n",
    "    return balanced_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/7wj664s92gx_89p1xhcwbq800000gn/T/ipykernel_24904/2291915788.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  audio_dataset = torch.load(\"../../Data/audio_test_dataset.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: {5: 70, 35: 37, 20: 83, 11: 42, 29: 100, 28: 56, 6: 67, 10: 58, 19: 100, 44: 31, 12: 68, 34: 92, 21: 58, 37: 100, 8: 34, 31: 40, 40: 35, 25: 42, 22: 62, 42: 42, 0: 33, 30: 100, 14: 52, 23: 100, 27: 31, 7: 100, 18: 100, 45: 66, 41: 30, 9: 37, 2: 57, 13: 60, 3: 67, 43: 45, 15: 37, 26: 36, 4: 73, 32: 57, 33: 34, 38: 42, 46: 67, 24: 41, 1: 48, 39: 32, 17: 39, 36: 39, 16: 42}\n",
      "Balanced class distribution: {5: 500, 35: 500, 20: 500, 11: 500, 29: 500, 28: 500, 6: 500, 10: 500, 19: 500, 44: 500, 12: 500, 34: 500, 21: 500, 37: 500, 8: 500, 31: 500, 40: 500, 25: 500, 22: 500, 42: 500, 0: 500, 30: 500, 14: 500, 23: 500, 27: 500, 7: 500, 18: 500, 45: 500, 41: 500, 9: 500, 2: 500, 13: 500, 3: 500, 43: 500, 15: 500, 26: 500, 4: 500, 32: 500, 33: 500, 38: 500, 46: 500, 24: 500, 1: 500, 39: 500, 17: 500, 36: 500, 16: 500}\n",
      "Size of the balanced dataset: 23500\n"
     ]
    }
   ],
   "source": [
    "# Load the audio test dataset from the .pt file.\n",
    "audio_dataset = torch.load(\"../../Data/audio_test_dataset.pt\")\n",
    "# Create a balanced dataset with 500 samples per class.\n",
    "balanced_audio_dataset = create_balanced_dataset(audio_dataset, target_samples_per_class=500)\n",
    "# Print out the size of the dataset.\n",
    "print(f\"Size of the balanced dataset: {len(balanced_audio_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/7wj664s92gx_89p1xhcwbq800000gn/T/ipykernel_24904/3125005379.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  images_dataset = torch.load(\"../../Data/images_test_dataset.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features tensor shape: torch.Size([821, 3, 224, 224])\n",
      "Labels tensor shape: torch.Size([821])\n"
     ]
    }
   ],
   "source": [
    "# Now load the image test dataset from the .pt file.\n",
    "images_dataset = torch.load(\"../../Data/images_test_dataset.pt\")\n",
    "# This is a dictionary with a list of tensors.\n",
    "# We want to extract the data into two tensors - features and labels.\n",
    "features = []\n",
    "for image_tensor in images_dataset[\"images\"]:\n",
    "    features.append(image_tensor)\n",
    "labels = []\n",
    "for label in images_dataset[\"labels\"]:\n",
    "    labels.append(label)\n",
    "# Convert the features and labels to PyTorch tensors.\n",
    "features_tensor = torch.stack(features)\n",
    "labels_tensor = torch.tensor(labels)\n",
    "\n",
    "print(f\"Features tensor shape: {features_tensor.shape}\")\n",
    "print(f\"Labels tensor shape: {labels_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: {796: 18, 810: 16, 875: 18, 774: 16, 513: 15, 756: 18, 889: 16, 753: 18, 831: 18, 987: 18, 886: 18, 400: 17, 802: 18, 766: 18, 852: 18, 848: 18, 877: 16, 352: 18, 964: 18, 749: 16, 888: 18, 315: 18, 543: 18, 790: 18, 957: 18, 871: 16, 669: 18, 317: 18, 851: 18, 900: 18, 910: 18, 746: 18, 536: 16, 832: 17, 949: 18, 902: 18, 856: 18, 835: 18, 847: 16, 979: 18, 914: 18, 539: 15, 950: 18, 450: 18, 527: 18, 946: 17, 830: 18}\n",
      "Balanced class distribution: {796: 500, 810: 500, 875: 500, 774: 500, 513: 500, 756: 500, 889: 500, 753: 500, 831: 500, 987: 500, 886: 500, 400: 500, 802: 500, 766: 500, 852: 500, 848: 500, 877: 500, 352: 500, 964: 500, 749: 500, 888: 500, 315: 500, 543: 500, 790: 500, 957: 500, 871: 500, 669: 500, 317: 500, 851: 500, 900: 500, 910: 500, 746: 500, 536: 500, 832: 500, 949: 500, 902: 500, 856: 500, 835: 500, 847: 500, 979: 500, 914: 500, 539: 500, 950: 500, 450: 500, 527: 500, 946: 500, 830: 500}\n",
      "Size of the balanced dataset: 23500\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset from the features and labels, and use it to create a balanced dataset.\n",
    "images_dataset = TensorDataset(features_tensor, labels_tensor)\n",
    "balanced_images_dataset = create_balanced_dataset(images_dataset, target_samples_per_class=500)\n",
    "# Print out the size of the dataset.\n",
    "print(f\"Size of the balanced dataset: {len(balanced_images_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Creating Combined Dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced images data tensor shape: torch.Size([23500, 3, 224, 224])\n",
      "Balanced images labels tensor shape: torch.Size([23500])\n"
     ]
    }
   ],
   "source": [
    "# Extract the images data from the balanced dataset.\n",
    "images_data = []\n",
    "images_labels = []\n",
    "for i in range(len(balanced_images_dataset)):\n",
    "    image, label = balanced_images_dataset[i]\n",
    "    images_data.append(image)\n",
    "    images_labels.append(label)\n",
    "# Convert the data and labels to PyTorch tensors.\n",
    "images_data_tensor = torch.stack(images_data)\n",
    "images_labels_tensor = torch.tensor(images_labels)\n",
    "print(f\"Balanced images data tensor shape: {images_data_tensor.shape}\")\n",
    "print(f\"Balanced images labels tensor shape: {images_labels_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced audio data tensor shape: torch.Size([23500, 1, 128, 128])\n",
      "Balanced audio labels tensor shape: torch.Size([23500])\n"
     ]
    }
   ],
   "source": [
    "# Extract the audio data from the balanced dataset.\n",
    "audio_data = []\n",
    "audio_labels = []\n",
    "for i in range(len(balanced_audio_dataset)):\n",
    "    audio, label = balanced_audio_dataset[i]\n",
    "    audio_data.append(audio)\n",
    "    audio_labels.append(label)\n",
    "# Convert the data and labels to PyTorch tensors.\n",
    "audio_data_tensor = torch.stack(audio_data)\n",
    "audio_labels_tensor = torch.tensor(audio_labels)\n",
    "print(f\"Balanced audio data tensor shape: {audio_data_tensor.shape}\")\n",
    "print(f\"Balanced audio labels tensor shape: {audio_labels_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECE421",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
