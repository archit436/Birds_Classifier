{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import TensorDataset, Subset, Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Loading Individual Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to use a dataset to create a balanced dataset that makes\n",
    "# sure each class has the same number of samples.\n",
    "def create_balanced_dataset(test_dataset, target_samples_per_class=500):\n",
    "    # Gather labels from the test dataset.\n",
    "    test_labels = []\n",
    "    for i in range(len(test_dataset)):\n",
    "        _, label = test_dataset[i]  \n",
    "        # Extract the label from the dataset item.\n",
    "        test_labels.append(label.item())\n",
    "\n",
    "    # Count samples per class in the test dataset\n",
    "    class_counts = Counter(test_labels)\n",
    "    print(f\"Original class distribution: {dict(class_counts)}\")\n",
    "\n",
    "    # Identify classes that need oversampling\n",
    "    classes_to_oversample = {\n",
    "        cls: (target_samples_per_class - count)\n",
    "        for cls, count in class_counts.items()\n",
    "        if count < target_samples_per_class\n",
    "    }\n",
    "\n",
    "    # If no class is under the target, simply return the original dataset\n",
    "    if not classes_to_oversample:\n",
    "        print(\"No oversampling needed - all classes have enough samples.\")\n",
    "        return test_dataset\n",
    "\n",
    "    # Map each class to the list of indices that contain that class\n",
    "    class_indices_map = {cls: [] for cls in class_counts.keys()}\n",
    "    for i, lbl in enumerate(test_labels):\n",
    "        class_indices_map[lbl].append(i)\n",
    "\n",
    "    # Generate the new indices by oversampling\n",
    "    additional_indices = []\n",
    "    for cls, num_needed in classes_to_oversample.items():\n",
    "        # Randomly sample (with replacement) from the available indices of this class\n",
    "        oversampled = np.random.choice(class_indices_map[cls], size=num_needed, replace=True)\n",
    "        additional_indices.extend(oversampled)\n",
    "\n",
    "    # Combine original indices with the newly oversampled ones\n",
    "    all_indices = list(range(len(test_dataset))) + additional_indices\n",
    "\n",
    "    # Create a new Subset using these indices\n",
    "    balanced_dataset = Subset(test_dataset, all_indices)\n",
    "\n",
    "    # Optional: verify the new distribution\n",
    "    balanced_labels = []\n",
    "    for idx in all_indices:\n",
    "        _, label = test_dataset[idx]\n",
    "        balanced_labels.append(int(label))\n",
    "    balanced_counts = Counter(balanced_labels)\n",
    "    print(f\"Balanced class distribution: {dict(balanced_counts)}\")\n",
    "\n",
    "    return balanced_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/7wj664s92gx_89p1xhcwbq800000gn/T/ipykernel_29800/2291915788.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  audio_dataset = torch.load(\"../../Data/audio_test_dataset.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: {5: 70, 35: 37, 20: 83, 11: 42, 29: 100, 28: 56, 6: 67, 10: 58, 19: 100, 44: 31, 12: 68, 34: 92, 21: 58, 37: 100, 8: 34, 31: 40, 40: 35, 25: 42, 22: 62, 42: 42, 0: 33, 30: 100, 14: 52, 23: 100, 27: 31, 7: 100, 18: 100, 45: 66, 41: 30, 9: 37, 2: 57, 13: 60, 3: 67, 43: 45, 15: 37, 26: 36, 4: 73, 32: 57, 33: 34, 38: 42, 46: 67, 24: 41, 1: 48, 39: 32, 17: 39, 36: 39, 16: 42}\n",
      "Balanced class distribution: {5: 500, 35: 500, 20: 500, 11: 500, 29: 500, 28: 500, 6: 500, 10: 500, 19: 500, 44: 500, 12: 500, 34: 500, 21: 500, 37: 500, 8: 500, 31: 500, 40: 500, 25: 500, 22: 500, 42: 500, 0: 500, 30: 500, 14: 500, 23: 500, 27: 500, 7: 500, 18: 500, 45: 500, 41: 500, 9: 500, 2: 500, 13: 500, 3: 500, 43: 500, 15: 500, 26: 500, 4: 500, 32: 500, 33: 500, 38: 500, 46: 500, 24: 500, 1: 500, 39: 500, 17: 500, 36: 500, 16: 500}\n",
      "Size of the balanced dataset: 23500\n"
     ]
    }
   ],
   "source": [
    "# Load the audio test dataset from the .pt file.\n",
    "audio_dataset = torch.load(\"../../Data/audio_test_dataset.pt\")\n",
    "# Create a balanced dataset with 500 samples per class.\n",
    "balanced_audio_dataset = create_balanced_dataset(audio_dataset, target_samples_per_class=500)\n",
    "# Print out the size of the dataset.\n",
    "print(f\"Size of the balanced dataset: {len(balanced_audio_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/7wj664s92gx_89p1xhcwbq800000gn/T/ipykernel_29800/1137858584.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  images_dataset = torch.load(\"../../Data/images_test_dataset.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features tensor shape: torch.Size([821, 3, 224, 224])\n",
      "Labels tensor shape: torch.Size([821])\n"
     ]
    }
   ],
   "source": [
    "# Now load the image test dataset from the .pt file.\n",
    "images_dataset = torch.load(\"../../Data/images_test_dataset.pt\")\n",
    "# This is a dictionary with a list of tensors.\n",
    "# We want to extract the data into two tensors - features and labels.\n",
    "features = []\n",
    "for image_tensor in images_dataset[\"images\"]:\n",
    "    features.append(image_tensor)\n",
    "labels = []\n",
    "for label in images_dataset[\"labels\"]:\n",
    "    labels.append(label)\n",
    "# Convert the features and labels to PyTorch tensors.\n",
    "features_tensor = torch.stack(features)\n",
    "labels_tensor = torch.tensor(labels)\n",
    "\n",
    "print(f\"Features tensor shape: {features_tensor.shape}\")\n",
    "print(f\"Labels tensor shape: {labels_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded labels tensor shape: torch.Size([821])\n"
     ]
    }
   ],
   "source": [
    "# We need to encode the labels of images, similar to how we did for audio.\n",
    "# Load the label mappings from the pickle file\n",
    "with open('label_mappings.pkl', 'rb') as f:\n",
    "    label_mappings = pickle.load(f)\n",
    "\n",
    "# Encode the labels using the mappings\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(list(label_mappings['index_to_id'].values()))\n",
    "encoded_labels = label_encoder.transform(labels_tensor.numpy())\n",
    "# Convert the encoded labels to a tensor\n",
    "encoded_labels_tensor = torch.tensor(encoded_labels)\n",
    "print(f\"Encoded labels tensor shape: {encoded_labels_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: {18: 18, 20: 16, 31: 18, 16: 16, 5: 15, 14: 18, 35: 16, 13: 18, 22: 18, 46: 18, 33: 18, 3: 17, 19: 18, 15: 18, 28: 18, 26: 18, 32: 16, 2: 18, 44: 18, 12: 16, 34: 18, 0: 18, 9: 18, 17: 18, 43: 18, 30: 16, 10: 18, 1: 18, 27: 18, 36: 18, 38: 18, 11: 18, 7: 16, 23: 17, 41: 18, 37: 18, 29: 18, 24: 18, 25: 16, 45: 18, 39: 18, 8: 15, 42: 18, 4: 18, 6: 18, 40: 17, 21: 18}\n",
      "Balanced class distribution: {18: 500, 20: 500, 31: 500, 16: 500, 5: 500, 14: 500, 35: 500, 13: 500, 22: 500, 46: 500, 33: 500, 3: 500, 19: 500, 15: 500, 28: 500, 26: 500, 32: 500, 2: 500, 44: 500, 12: 500, 34: 500, 0: 500, 9: 500, 17: 500, 43: 500, 30: 500, 10: 500, 1: 500, 27: 500, 36: 500, 38: 500, 11: 500, 7: 500, 23: 500, 41: 500, 37: 500, 29: 500, 24: 500, 25: 500, 45: 500, 39: 500, 8: 500, 42: 500, 4: 500, 6: 500, 40: 500, 21: 500}\n",
      "Size of the balanced dataset: 23500\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset from the features and labels, and use it to create a balanced dataset.\n",
    "images_dataset = TensorDataset(features_tensor, encoded_labels_tensor)\n",
    "balanced_images_dataset = create_balanced_dataset(images_dataset, target_samples_per_class=500)\n",
    "# Print out the size of the dataset.\n",
    "print(f\"Size of the balanced dataset: {len(balanced_images_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Creating Combined Dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images data tensor shape: torch.Size([23500, 3, 224, 224])\n",
      "Images labels tensor shape: torch.Size([23500])\n"
     ]
    }
   ],
   "source": [
    "# Extract the images data and labels from the balanced dataset\n",
    "images_data = []\n",
    "images_labels = []\n",
    "for i in range(len(balanced_images_dataset)):\n",
    "    image, label = balanced_images_dataset[i]\n",
    "    images_data.append(image)\n",
    "    images_labels.append(label)\n",
    "# Convert to tensors\n",
    "images_data_tensor = torch.stack(images_data)\n",
    "images_labels_tensor = torch.tensor(images_labels)\n",
    "print(f\"Images data tensor shape: {images_data_tensor.shape}\")\n",
    "print(f\"Images labels tensor shape: {images_labels_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced audio data tensor shape: torch.Size([23500, 1, 128, 128])\n",
      "Balanced audio labels tensor shape: torch.Size([23500])\n"
     ]
    }
   ],
   "source": [
    "# Extract the audio data from the balanced dataset.\n",
    "audio_data = []\n",
    "audio_labels = []\n",
    "for i in range(len(balanced_audio_dataset)):\n",
    "    audio, label = balanced_audio_dataset[i]\n",
    "    audio_data.append(audio)\n",
    "    audio_labels.append(label)\n",
    "# Convert the data and labels to PyTorch tensors.\n",
    "audio_data_tensor = torch.stack(audio_data)\n",
    "audio_labels_tensor = torch.tensor(audio_labels)\n",
    "print(f\"Balanced audio data tensor shape: {audio_data_tensor.shape}\")\n",
    "print(f\"Balanced audio labels tensor shape: {audio_labels_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a class to create a dataset for the audio and image data.\n",
    "# We want to create random pairs of audio and image data, of the same class.\n",
    "class MultimodalFusionDataset(Dataset):\n",
    "    def __init__(self, image_data, image_labels, audio_data, audio_labels, transform=None, indices=None):\n",
    "        \"\"\"\n",
    "        Custom dataset for multimodal fusion of image and audio data.\n",
    "        \n",
    "        Args:\n",
    "            image_data: PyTorch tensor containing image data\n",
    "            image_labels: PyTorch tensor containing image labels\n",
    "            audio_data: PyTorch tensor containing audio data\n",
    "            audio_labels: PyTorch tensor containing audio labels\n",
    "            transform: Optional transform to be applied to the samples\n",
    "            indices: Optional indices to select a subset of the data\n",
    "        \"\"\"\n",
    "        self.image_data = image_data\n",
    "        self.image_labels = image_labels\n",
    "        self.audio_data = audio_data\n",
    "        self.audio_labels = audio_labels\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Convert tensors to numpy for processing\n",
    "        image_labels_np = image_labels.cpu().numpy()\n",
    "        audio_labels_np = audio_labels.cpu().numpy()\n",
    "        \n",
    "        # Create a list of all possible class indices - ensure they're the same for both modalities\n",
    "        image_classes = set(np.unique(image_labels_np))\n",
    "        audio_classes = set(np.unique(audio_labels_np))\n",
    "        common_classes = sorted(list(image_classes.intersection(audio_classes)))\n",
    "        \n",
    "        if len(common_classes) == 0:\n",
    "            raise ValueError(\"No common classes found between image and audio datasets\")\n",
    "        \n",
    "        # Organize samples by class\n",
    "        self.class_indices = {}\n",
    "        self.pairs = []\n",
    "        self.pair_labels = []\n",
    "        \n",
    "        for cls in common_classes:\n",
    "            img_indices = np.where(image_labels_np == cls)[0]\n",
    "            audio_indices = np.where(audio_labels_np == cls)[0]\n",
    "            \n",
    "            if len(img_indices) > 0 and len(audio_indices) > 0:\n",
    "                self.class_indices[cls] = {\n",
    "                    'image': img_indices,\n",
    "                    'audio': audio_indices\n",
    "                }\n",
    "                \n",
    "                # Shuffle the indices for random pairing\n",
    "                np.random.seed(42)  # For reproducibility\n",
    "                np.random.shuffle(img_indices)\n",
    "                np.random.shuffle(audio_indices)\n",
    "                \n",
    "                # Create pairs (one image with one audio) from the same class\n",
    "                n_pairs = min(len(img_indices), len(audio_indices))\n",
    "                for i in range(n_pairs):\n",
    "                    self.pairs.append((img_indices[i], audio_indices[i]))\n",
    "                    self.pair_labels.append(cls)\n",
    "        \n",
    "        # Check if we have any pairs\n",
    "        if len(self.pairs) == 0:\n",
    "            raise ValueError(\"No valid pairs could be created. Check your data and labels.\")\n",
    "        \n",
    "        # Convert to numpy arrays for easier indexing\n",
    "        self.pairs = np.array(self.pairs)\n",
    "        self.pair_labels = np.array(self.pair_labels)\n",
    "        \n",
    "        # If specific indices are provided, only use those\n",
    "        if indices is not None:\n",
    "            if len(indices) > 0:  # Make sure indices is not empty\n",
    "                self.pairs = self.pairs[indices]\n",
    "                self.pair_labels = self.pair_labels[indices]\n",
    "            else:\n",
    "                raise ValueError(\"Empty indices provided\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_idx, audio_idx = self.pairs[idx]\n",
    "        \n",
    "        image = self.image_data[img_idx]\n",
    "        audio = self.audio_data[audio_idx]\n",
    "        label = torch.tensor(self.pair_labels[idx], dtype=torch.long)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return {\n",
    "            'image': image,\n",
    "            'audio': audio,\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "\n",
    "# Create stratified data loaders for training, validation, and testing\n",
    "# These will be created using the dataset framework we defined above,\n",
    "# and using the balanced datasets we created earlier.\n",
    "def create_stratified_data_loaders(image_data, image_labels, audio_data, audio_labels, batch_size=32, num_workers=4):\n",
    "    \"\"\"\n",
    "    Create train, validation, and test data loaders with stratified splits.\n",
    "    \n",
    "    Args:\n",
    "        image_data: PyTorch tensor containing image data\n",
    "        image_labels: PyTorch tensor containing image labels\n",
    "        audio_data: PyTorch tensor containing audio data\n",
    "        audio_labels: PyTorch tensor containing audio labels\n",
    "        batch_size: Batch size for the data loaders\n",
    "        num_workers: Number of worker threads for the data loaders\n",
    "        \n",
    "    Returns:\n",
    "        train_loader, val_loader, test_loader\n",
    "    \"\"\"\n",
    "    # First, create the full dataset\n",
    "    try:\n",
    "        full_dataset = MultimodalFusionDataset(\n",
    "            image_data=image_data,\n",
    "            image_labels=image_labels,\n",
    "            audio_data=audio_data,\n",
    "            audio_labels=audio_labels\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(f\"Error creating dataset: {e}\")\n",
    "        print(f\"Image labels shape: {image_labels.shape}, unique: {torch.unique(image_labels).shape}\")\n",
    "        print(f\"Audio labels shape: {audio_labels.shape}, unique: {torch.unique(audio_labels).shape}\")\n",
    "        raise\n",
    "    \n",
    "    # Get all pair labels\n",
    "    pair_labels = full_dataset.pair_labels\n",
    "    \n",
    "    # Create indices array\n",
    "    indices = np.arange(len(pair_labels))\n",
    "    \n",
    "    # First split: train+val vs test (80:20)\n",
    "    sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "    temp_idx, test_idx = next(sss1.split(indices, pair_labels))\n",
    "    \n",
    "    # Get the temporary set labels\n",
    "    temp_labels = pair_labels[temp_idx]\n",
    "    \n",
    "    # Second split: train vs val (80:20)\n",
    "    temp_indices = np.arange(len(temp_idx))\n",
    "    sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "    train_temp_idx, val_temp_idx = next(sss2.split(temp_indices, temp_labels))\n",
    "    \n",
    "    # Convert to original indices\n",
    "    train_idx = temp_idx[train_temp_idx]\n",
    "    val_idx = temp_idx[val_temp_idx]\n",
    "    \n",
    "    # Create the individual datasets\n",
    "    train_dataset = MultimodalFusionDataset(\n",
    "        image_data=image_data,\n",
    "        image_labels=image_labels,\n",
    "        audio_data=audio_data,\n",
    "        audio_labels=audio_labels,\n",
    "        indices=train_idx\n",
    "    )\n",
    "    \n",
    "    val_dataset = MultimodalFusionDataset(\n",
    "        image_data=image_data,\n",
    "        image_labels=image_labels,\n",
    "        audio_data=audio_data,\n",
    "        audio_labels=audio_labels,\n",
    "        indices=val_idx\n",
    "    )\n",
    "    \n",
    "    test_dataset = MultimodalFusionDataset(\n",
    "        image_data=image_data,\n",
    "        image_labels=image_labels,\n",
    "        audio_data=audio_data,\n",
    "        audio_labels=audio_labels,\n",
    "        indices=test_idx\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    # Print split information\n",
    "    print(f\"Dataset split: {len(train_dataset)} training, {len(val_dataset)} validation, {len(test_dataset)} test samples\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split: 15040 training, 3760 validation, 4700 test samples\n"
     ]
    }
   ],
   "source": [
    "# Use the extracted tensors to create stratified data loaders\n",
    "batch_size = 32\n",
    "train_loader, val_loader, test_loader = create_stratified_data_loaders(\n",
    "    image_data=images_data_tensor,\n",
    "    image_labels=images_labels_tensor,\n",
    "    audio_data=audio_data_tensor,\n",
    "    audio_labels=audio_labels_tensor,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Fusion Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by defining the Fusion ANN Model.\n",
    "class FusionANN(nn.Module):\n",
    "    def __init__(self, image_embed_size, audio_embed_size, hidden_dim, num_classes, dropout_rate=0.5):\n",
    "        super(FusionANN, self).__init__()\n",
    "        \n",
    "        # Dimensionality of inputs\n",
    "        self.image_embed_size = image_embed_size\n",
    "        self.audio_embed_size = audio_embed_size\n",
    "        \n",
    "        # Fusion network\n",
    "        self.fusion_network = nn.Sequential(\n",
    "            nn.Linear(image_embed_size + audio_embed_size, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, image_features, audio_features):\n",
    "        # Concatenate features from both modalities\n",
    "        combined_features = torch.cat((image_features, audio_features), dim=1)\n",
    "        \n",
    "        # Pass through fusion network\n",
    "        output = self.fusion_network(combined_features)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load pre-trained models\n",
    "def load_pretrained_models(image_model_path, audio_model_path, device):\n",
    "    # Load image model\n",
    "    image_model = torch.load(image_model_path, map_location=device)\n",
    "    image_model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    # Load audio model\n",
    "    audio_model = torch.load(audio_model_path, map_location=device)\n",
    "    audio_model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    return image_model, audio_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from the pre-trained models\n",
    "def extract_features(image_model, audio_model, dataloader, device):\n",
    "    image_features_list = []\n",
    "    audio_features_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Extracting features\"):\n",
    "            images = batch['image'].to(device)\n",
    "            audio = batch['audio'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Extract features from the pre-trained models\n",
    "            # Note: Assuming the models return features before the final classification layer\n",
    "            # You may need to modify this based on your model architecture\n",
    "            image_features = image_model(images)\n",
    "            audio_features = audio_model(audio)\n",
    "            \n",
    "            image_features_list.append(image_features)\n",
    "            audio_features_list.append(audio_features)\n",
    "            labels_list.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the fusion model\n",
    "def train_fusion_model(fusion_model, train_image_features, train_audio_features, train_labels,\n",
    "                       val_image_features, val_audio_features, val_labels,\n",
    "                       num_epochs, batch_size, learning_rate, device):\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(fusion_model.parameters(), lr=learning_rate)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
    "    \n",
    "    # Training history\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    # Number of batches\n",
    "    num_train_samples = train_image_features.shape[0]\n",
    "    num_batches = (num_train_samples + batch_size - 1) // batch_size\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        fusion_model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        \n",
    "        # Create random permutation for shuffling\n",
    "        indices = torch.randperm(num_train_samples)\n",
    "        \n",
    "        for batch_idx in tqdm(range(num_batches), desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "            # Get batch indices\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min((batch_idx + 1) * batch_size, num_train_samples)\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "            \n",
    "            # Get batch data\n",
    "            batch_image_features = train_image_features[batch_indices].to(device)\n",
    "            batch_audio_features = train_audio_features[batch_indices].to(device)\n",
    "            batch_labels = train_labels[batch_indices].to(device)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = fusion_model(batch_image_features, batch_audio_features)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update statistics\n",
    "            running_loss += loss.item() * batch_indices.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_train += (predicted == batch_labels).sum().item()\n",
    "        \n",
    "        # Calculate training statistics for the epoch\n",
    "        train_loss = running_loss / num_train_samples\n",
    "        train_accuracy = correct_train / num_train_samples\n",
    "        \n",
    "        # Validation phase\n",
    "        fusion_model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = fusion_model(val_image_features.to(device), val_audio_features.to(device))\n",
    "            val_loss = criterion(val_outputs, val_labels.to(device)).item()\n",
    "            _, val_predicted = torch.max(val_outputs, 1)\n",
    "            val_accuracy = (val_predicted == val_labels.to(device)).sum().item() / val_labels.size(0)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Save stats\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        # Print statistics\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_accuracy > best_val_acc:\n",
    "            best_val_acc = val_accuracy\n",
    "            best_model_state = fusion_model.state_dict().copy()\n",
    "            print(f\"New best model with validation accuracy: {best_val_acc:.4f}\")\n",
    "    \n",
    "    # Load best model\n",
    "    fusion_model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Return training history\n",
    "    history = {\n",
    "        'train_loss': train_losses,\n",
    "        'val_loss': val_losses,\n",
    "        'train_acc': train_accuracies,\n",
    "        'val_acc': val_accuracies\n",
    "    }\n",
    "    \n",
    "    return fusion_model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model on the test set\n",
    "def evaluate_model(fusion_model, test_image_features, test_audio_features, test_labels, device):\n",
    "    fusion_model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        outputs = fusion_model(test_image_features.to(device), test_audio_features.to(device))\n",
    "        \n",
    "        # Get predictions\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_accuracy = (predicted == test_labels.to(device)).sum().item() / test_labels.size(0)\n",
    "        \n",
    "        # Calculate per-class accuracy\n",
    "        num_classes = outputs.size(1)\n",
    "        class_correct = [0] * num_classes\n",
    "        class_total = [0] * num_classes\n",
    "        \n",
    "        for i in range(test_labels.size(0)):\n",
    "            label = test_labels[i].item()\n",
    "            class_correct[label] += (predicted[i] == label).item()\n",
    "            class_total[label] += 1\n",
    "        \n",
    "        class_accuracy = [class_correct[i] / (class_total[i] + 1e-8) for i in range(num_classes)]\n",
    "    \n",
    "    return test_accuracy, class_accuracy\n",
    "\n",
    "# Function to plot training history\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fusion_model_training_history.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "\"\"\"FOR MACBOOK LOCAL SETUP USERS ONLY \"\"\"\n",
    "use_mps = True\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"mps\") if use_mps and torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/7wj664s92gx_89p1xhcwbq800000gn/T/ipykernel_29800/2144216013.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  image_model = torch.load(image_model_path, map_location=device)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m image_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModels/dummy_image_model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m audio_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModels/dummy_audio_model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m image_model, audio_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_pretrained_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[50], line 5\u001b[0m, in \u001b[0;36mload_pretrained_models\u001b[0;34m(image_model_path, audio_model_path, device)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_pretrained_models\u001b[39m(image_model_path, audio_model_path, device):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Load image model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     image_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(image_model_path, map_location\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mimage_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m()  \u001b[38;5;66;03m# Set to evaluation mode\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Load audio model\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     audio_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(audio_model_path, map_location\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "# Now we run and train the model.\n",
    "# Load pre-trained models\n",
    "image_model_path = \"Models/dummy_image_model.pt\"\n",
    "audio_model_path = \"Models/dummy_audio_model.pt\"\n",
    "image_model, audio_model = load_pretrained_models(image_model_path, audio_model_path, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECE421",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
