{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import TensorDataset, Subset, Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Loading Individual Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to use a dataset to create a balanced dataset that makes\n",
    "# sure each class has the same number of samples.\n",
    "def create_balanced_dataset(test_dataset, target_samples_per_class=500):\n",
    "    # Gather labels from the test dataset.\n",
    "    test_labels = []\n",
    "    for i in range(len(test_dataset)):\n",
    "        _, label = test_dataset[i]  \n",
    "        # Extract the label from the dataset item.\n",
    "        test_labels.append(label.item())\n",
    "\n",
    "    # Count samples per class in the test dataset\n",
    "    class_counts = Counter(test_labels)\n",
    "    print(f\"Original class distribution: {dict(class_counts)}\")\n",
    "\n",
    "    # Identify classes that need oversampling\n",
    "    classes_to_oversample = {\n",
    "        cls: (target_samples_per_class - count)\n",
    "        for cls, count in class_counts.items()\n",
    "        if count < target_samples_per_class\n",
    "    }\n",
    "\n",
    "    # If no class is under the target, simply return the original dataset\n",
    "    if not classes_to_oversample:\n",
    "        print(\"No oversampling needed - all classes have enough samples.\")\n",
    "        return test_dataset\n",
    "\n",
    "    # Map each class to the list of indices that contain that class\n",
    "    class_indices_map = {cls: [] for cls in class_counts.keys()}\n",
    "    for i, lbl in enumerate(test_labels):\n",
    "        class_indices_map[lbl].append(i)\n",
    "\n",
    "    # Generate the new indices by oversampling\n",
    "    additional_indices = []\n",
    "    for cls, num_needed in classes_to_oversample.items():\n",
    "        # Randomly sample (with replacement) from the available indices of this class\n",
    "        oversampled = np.random.choice(class_indices_map[cls], size=num_needed, replace=True)\n",
    "        additional_indices.extend(oversampled)\n",
    "\n",
    "    # Combine original indices with the newly oversampled ones\n",
    "    all_indices = list(range(len(test_dataset))) + additional_indices\n",
    "\n",
    "    # Create a new Subset using these indices\n",
    "    balanced_dataset = Subset(test_dataset, all_indices)\n",
    "\n",
    "    # Optional: verify the new distribution\n",
    "    balanced_labels = []\n",
    "    for idx in all_indices:\n",
    "        _, label = test_dataset[idx]\n",
    "        balanced_labels.append(int(label))\n",
    "    balanced_counts = Counter(balanced_labels)\n",
    "    print(f\"Balanced class distribution: {dict(balanced_counts)}\")\n",
    "\n",
    "    return balanced_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/7wj664s92gx_89p1xhcwbq800000gn/T/ipykernel_29800/2291915788.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  audio_dataset = torch.load(\"../../Data/audio_test_dataset.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: {5: 70, 35: 37, 20: 83, 11: 42, 29: 100, 28: 56, 6: 67, 10: 58, 19: 100, 44: 31, 12: 68, 34: 92, 21: 58, 37: 100, 8: 34, 31: 40, 40: 35, 25: 42, 22: 62, 42: 42, 0: 33, 30: 100, 14: 52, 23: 100, 27: 31, 7: 100, 18: 100, 45: 66, 41: 30, 9: 37, 2: 57, 13: 60, 3: 67, 43: 45, 15: 37, 26: 36, 4: 73, 32: 57, 33: 34, 38: 42, 46: 67, 24: 41, 1: 48, 39: 32, 17: 39, 36: 39, 16: 42}\n",
      "Balanced class distribution: {5: 500, 35: 500, 20: 500, 11: 500, 29: 500, 28: 500, 6: 500, 10: 500, 19: 500, 44: 500, 12: 500, 34: 500, 21: 500, 37: 500, 8: 500, 31: 500, 40: 500, 25: 500, 22: 500, 42: 500, 0: 500, 30: 500, 14: 500, 23: 500, 27: 500, 7: 500, 18: 500, 45: 500, 41: 500, 9: 500, 2: 500, 13: 500, 3: 500, 43: 500, 15: 500, 26: 500, 4: 500, 32: 500, 33: 500, 38: 500, 46: 500, 24: 500, 1: 500, 39: 500, 17: 500, 36: 500, 16: 500}\n",
      "Size of the balanced dataset: 23500\n"
     ]
    }
   ],
   "source": [
    "# Load the audio test dataset from the .pt file.\n",
    "audio_dataset = torch.load(\"../../Data/audio_test_dataset.pt\")\n",
    "# Create a balanced dataset with 500 samples per class.\n",
    "balanced_audio_dataset = create_balanced_dataset(audio_dataset, target_samples_per_class=500)\n",
    "# Print out the size of the dataset.\n",
    "print(f\"Size of the balanced dataset: {len(balanced_audio_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/7wj664s92gx_89p1xhcwbq800000gn/T/ipykernel_29800/1137858584.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  images_dataset = torch.load(\"../../Data/images_test_dataset.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features tensor shape: torch.Size([821, 3, 224, 224])\n",
      "Labels tensor shape: torch.Size([821])\n"
     ]
    }
   ],
   "source": [
    "# Now load the image test dataset from the .pt file.\n",
    "images_dataset = torch.load(\"../../Data/images_test_dataset.pt\")\n",
    "# This is a dictionary with a list of tensors.\n",
    "# We want to extract the data into two tensors - features and labels.\n",
    "features = []\n",
    "for image_tensor in images_dataset[\"images\"]:\n",
    "    features.append(image_tensor)\n",
    "labels = []\n",
    "for label in images_dataset[\"labels\"]:\n",
    "    labels.append(label)\n",
    "# Convert the features and labels to PyTorch tensors.\n",
    "features_tensor = torch.stack(features)\n",
    "labels_tensor = torch.tensor(labels)\n",
    "\n",
    "print(f\"Features tensor shape: {features_tensor.shape}\")\n",
    "print(f\"Labels tensor shape: {labels_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded labels tensor shape: torch.Size([821])\n"
     ]
    }
   ],
   "source": [
    "# We need to encode the labels of images, similar to how we did for audio.\n",
    "# Load the label mappings from the pickle file\n",
    "with open('label_mappings.pkl', 'rb') as f:\n",
    "    label_mappings = pickle.load(f)\n",
    "\n",
    "# Encode the labels using the mappings\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(list(label_mappings['index_to_id'].values()))\n",
    "encoded_labels = label_encoder.transform(labels_tensor.numpy())\n",
    "# Convert the encoded labels to a tensor\n",
    "encoded_labels_tensor = torch.tensor(encoded_labels)\n",
    "print(f\"Encoded labels tensor shape: {encoded_labels_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: {18: 18, 20: 16, 31: 18, 16: 16, 5: 15, 14: 18, 35: 16, 13: 18, 22: 18, 46: 18, 33: 18, 3: 17, 19: 18, 15: 18, 28: 18, 26: 18, 32: 16, 2: 18, 44: 18, 12: 16, 34: 18, 0: 18, 9: 18, 17: 18, 43: 18, 30: 16, 10: 18, 1: 18, 27: 18, 36: 18, 38: 18, 11: 18, 7: 16, 23: 17, 41: 18, 37: 18, 29: 18, 24: 18, 25: 16, 45: 18, 39: 18, 8: 15, 42: 18, 4: 18, 6: 18, 40: 17, 21: 18}\n",
      "Balanced class distribution: {18: 500, 20: 500, 31: 500, 16: 500, 5: 500, 14: 500, 35: 500, 13: 500, 22: 500, 46: 500, 33: 500, 3: 500, 19: 500, 15: 500, 28: 500, 26: 500, 32: 500, 2: 500, 44: 500, 12: 500, 34: 500, 0: 500, 9: 500, 17: 500, 43: 500, 30: 500, 10: 500, 1: 500, 27: 500, 36: 500, 38: 500, 11: 500, 7: 500, 23: 500, 41: 500, 37: 500, 29: 500, 24: 500, 25: 500, 45: 500, 39: 500, 8: 500, 42: 500, 4: 500, 6: 500, 40: 500, 21: 500}\n",
      "Size of the balanced dataset: 23500\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset from the features and labels, and use it to create a balanced dataset.\n",
    "images_dataset = TensorDataset(features_tensor, encoded_labels_tensor)\n",
    "balanced_images_dataset = create_balanced_dataset(images_dataset, target_samples_per_class=500)\n",
    "# Print out the size of the dataset.\n",
    "print(f\"Size of the balanced dataset: {len(balanced_images_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Creating Combined Dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced audio data tensor shape: torch.Size([23500, 1, 128, 128])\n",
      "Balanced audio labels tensor shape: torch.Size([23500])\n"
     ]
    }
   ],
   "source": [
    "# Extract the audio data from the balanced dataset.\n",
    "audio_data = []\n",
    "audio_labels = []\n",
    "for i in range(len(balanced_audio_dataset)):\n",
    "    audio, label = balanced_audio_dataset[i]\n",
    "    audio_data.append(audio)\n",
    "    audio_labels.append(label)\n",
    "# Convert the data and labels to PyTorch tensors.\n",
    "audio_data_tensor = torch.stack(audio_data)\n",
    "audio_labels_tensor = torch.tensor(audio_labels)\n",
    "print(f\"Balanced audio data tensor shape: {audio_data_tensor.shape}\")\n",
    "print(f\"Balanced audio labels tensor shape: {audio_labels_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a class to create a dataset for the audio and image data.\n",
    "# We want to create random pairs of audio and image data, of the same class.\n",
    "\n",
    "# Create stratified data loaders for training, validation, and testing\n",
    "# These will be created using the dataset framework we defined above,\n",
    "# and using the balanced datasets we created earlier.\n",
    "class MultimodalFusionDataset(Dataset):\n",
    "    def __init__(self, image_data, image_labels, audio_data, audio_labels, transform=None, indices=None):\n",
    "        \"\"\"\n",
    "        Custom dataset for multimodal fusion of image and audio data.\n",
    "        \n",
    "        Args:\n",
    "            image_data: PyTorch tensor containing image data\n",
    "            image_labels: PyTorch tensor containing image labels\n",
    "            audio_data: PyTorch tensor containing audio data\n",
    "            audio_labels: PyTorch tensor containing audio labels\n",
    "            transform: Optional transform to be applied to the samples\n",
    "            indices: Optional indices to select a subset of the data\n",
    "        \"\"\"\n",
    "        self.image_data = image_data\n",
    "        self.image_labels = image_labels\n",
    "        self.audio_data = audio_data\n",
    "        self.audio_labels = audio_labels\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Convert tensors to numpy for processing\n",
    "        image_labels_np = image_labels.cpu().numpy()\n",
    "        audio_labels_np = audio_labels.cpu().numpy()\n",
    "        \n",
    "        # Create a list of all possible class indices - ensure they're the same for both modalities\n",
    "        image_classes = set(np.unique(image_labels_np))\n",
    "        audio_classes = set(np.unique(audio_labels_np))\n",
    "        common_classes = sorted(list(image_classes.intersection(audio_classes)))\n",
    "        \n",
    "        if len(common_classes) == 0:\n",
    "            raise ValueError(\"No common classes found between image and audio datasets\")\n",
    "        \n",
    "        # Organize samples by class\n",
    "        self.class_indices = {}\n",
    "        self.pairs = []\n",
    "        self.pair_labels = []\n",
    "        \n",
    "        for cls in common_classes:\n",
    "            img_indices = np.where(image_labels_np == cls)[0]\n",
    "            audio_indices = np.where(audio_labels_np == cls)[0]\n",
    "            \n",
    "            if len(img_indices) > 0 and len(audio_indices) > 0:\n",
    "                self.class_indices[cls] = {\n",
    "                    'image': img_indices,\n",
    "                    'audio': audio_indices\n",
    "                }\n",
    "                \n",
    "                # Shuffle the indices for random pairing\n",
    "                np.random.seed(42)  # For reproducibility\n",
    "                np.random.shuffle(img_indices)\n",
    "                np.random.shuffle(audio_indices)\n",
    "                \n",
    "                # Create pairs (one image with one audio) from the same class\n",
    "                n_pairs = min(len(img_indices), len(audio_indices))\n",
    "                for i in range(n_pairs):\n",
    "                    self.pairs.append((img_indices[i], audio_indices[i]))\n",
    "                    self.pair_labels.append(cls)\n",
    "        \n",
    "        # Check if we have any pairs\n",
    "        if len(self.pairs) == 0:\n",
    "            raise ValueError(\"No valid pairs could be created. Check your data and labels.\")\n",
    "        \n",
    "        # Convert to numpy arrays for easier indexing\n",
    "        self.pairs = np.array(self.pairs)\n",
    "        self.pair_labels = np.array(self.pair_labels)\n",
    "        \n",
    "        # If specific indices are provided, only use those\n",
    "        if indices is not None:\n",
    "            if len(indices) > 0:  # Make sure indices is not empty\n",
    "                self.pairs = self.pairs[indices]\n",
    "                self.pair_labels = self.pair_labels[indices]\n",
    "            else:\n",
    "                raise ValueError(\"Empty indices provided\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_idx, audio_idx = self.pairs[idx]\n",
    "        \n",
    "        image = self.image_data[img_idx]\n",
    "        audio = self.audio_data[audio_idx]\n",
    "        label = torch.tensor(self.pair_labels[idx], dtype=torch.long)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return {\n",
    "            'image': image,\n",
    "            'audio': audio,\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "\n",
    "def create_stratified_data_loaders(image_data, image_labels, audio_data, audio_labels, batch_size=32, num_workers=4):\n",
    "    \"\"\"\n",
    "    Create train, validation, and test data loaders with stratified splits.\n",
    "    \n",
    "    Args:\n",
    "        image_data: PyTorch tensor containing image data\n",
    "        image_labels: PyTorch tensor containing image labels\n",
    "        audio_data: PyTorch tensor containing audio data\n",
    "        audio_labels: PyTorch tensor containing audio labels\n",
    "        batch_size: Batch size for the data loaders\n",
    "        num_workers: Number of worker threads for the data loaders\n",
    "        \n",
    "    Returns:\n",
    "        train_loader, val_loader, test_loader\n",
    "    \"\"\"\n",
    "    # First, create the full dataset\n",
    "    try:\n",
    "        full_dataset = MultimodalFusionDataset(\n",
    "            image_data=image_data,\n",
    "            image_labels=image_labels,\n",
    "            audio_data=audio_data,\n",
    "            audio_labels=audio_labels\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(f\"Error creating dataset: {e}\")\n",
    "        print(f\"Image labels shape: {image_labels.shape}, unique: {torch.unique(image_labels).shape}\")\n",
    "        print(f\"Audio labels shape: {audio_labels.shape}, unique: {torch.unique(audio_labels).shape}\")\n",
    "        raise\n",
    "    \n",
    "    # Get all pair labels\n",
    "    pair_labels = full_dataset.pair_labels\n",
    "    \n",
    "    # Create indices array\n",
    "    indices = np.arange(len(pair_labels))\n",
    "    \n",
    "    # First split: train+val vs test (80:20)\n",
    "    sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "    temp_idx, test_idx = next(sss1.split(indices, pair_labels))\n",
    "    \n",
    "    # Get the temporary set labels\n",
    "    temp_labels = pair_labels[temp_idx]\n",
    "    \n",
    "    # Second split: train vs val (80:20)\n",
    "    temp_indices = np.arange(len(temp_idx))\n",
    "    sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "    train_temp_idx, val_temp_idx = next(sss2.split(temp_indices, temp_labels))\n",
    "    \n",
    "    # Convert to original indices\n",
    "    train_idx = temp_idx[train_temp_idx]\n",
    "    val_idx = temp_idx[val_temp_idx]\n",
    "    \n",
    "    # Create the individual datasets\n",
    "    train_dataset = MultimodalFusionDataset(\n",
    "        image_data=image_data,\n",
    "        image_labels=image_labels,\n",
    "        audio_data=audio_data,\n",
    "        audio_labels=audio_labels,\n",
    "        indices=train_idx\n",
    "    )\n",
    "    \n",
    "    val_dataset = MultimodalFusionDataset(\n",
    "        image_data=image_data,\n",
    "        image_labels=image_labels,\n",
    "        audio_data=audio_data,\n",
    "        audio_labels=audio_labels,\n",
    "        indices=val_idx\n",
    "    )\n",
    "    \n",
    "    test_dataset = MultimodalFusionDataset(\n",
    "        image_data=image_data,\n",
    "        image_labels=image_labels,\n",
    "        audio_data=audio_data,\n",
    "        audio_labels=audio_labels,\n",
    "        indices=test_idx\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    # Print split information\n",
    "    print(f\"Dataset split: {len(train_dataset)} training, {len(val_dataset)} validation, {len(test_dataset)} test samples\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating dataset: No common classes found between image and audio datasets\n",
      "Image labels shape: torch.Size([23500]), unique: torch.Size([47])\n",
      "Audio labels shape: torch.Size([23500]), unique: torch.Size([47])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No common classes found between image and audio datasets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Use the extracted tensors to create stratified data loaders\u001b[39;00m\n\u001b[1;32m      2\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m----> 3\u001b[0m train_loader, val_loader, test_loader \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_stratified_data_loaders\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages_data_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages_labels_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_data_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_labels_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 117\u001b[0m, in \u001b[0;36mcreate_stratified_data_loaders\u001b[0;34m(image_data, image_labels, audio_data, audio_labels, batch_size, num_workers)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# First, create the full dataset\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     full_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mMultimodalFusionDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43maudio_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43maudio_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_labels\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError creating dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 36\u001b[0m, in \u001b[0;36mMultimodalFusionDataset.__init__\u001b[0;34m(self, image_data, image_labels, audio_data, audio_labels, transform, indices)\u001b[0m\n\u001b[1;32m     33\u001b[0m common_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(image_classes\u001b[38;5;241m.\u001b[39mintersection(audio_classes)))\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(common_classes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo common classes found between image and audio datasets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Organize samples by class\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_indices \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mValueError\u001b[0m: No common classes found between image and audio datasets"
     ]
    }
   ],
   "source": [
    "# Use the extracted tensors to create stratified data loaders\n",
    " -٠٩batch_size = 32\n",
    "train_loader, val_loader, test_loader = create_stratified_data_loaders(\n",
    "    image_data=images_data_tensor,\n",
    "    image_labels=images_labels_tensor,\n",
    "    audio_data=audio_data_tensor,\n",
    "    audio_labels=audio_labels_tensor,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECE421",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
