{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/archit436/Birds_Classifier/blob/main/Models/Main_Models/Main_Model_Audio_Main_File.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2Z5OlZ26H5S"
   },
   "source": [
    "0. Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2v-te8X34pLq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ECE421/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Start by importing the relevant libraries.\n",
    "# Copied from Archit's Lab 3 Submission and then some more.\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2FeatureExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Processing - WAV Tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/7wj664s92gx_89p1xhcwbq800000gn/T/ipykernel_7878/1743723501.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  features_tensor = torch.load(pt_file[0])\n"
     ]
    }
   ],
   "source": [
    "# Start by importing the tensors stored in the .pt files, one for each class.\n",
    "# Define the directory of the audio data.\n",
    "wav_tensors_dir = '../../Data/Xeno_Canto_WAV_Tensors'\n",
    "# Define a list to store the tensors and associated labels.\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "# Iterate through each bird.\n",
    "for file in os.listdir(wav_tensors_dir):\n",
    "    # Check if the file is a .pt file.\n",
    "    if(file.endswith('.pt') == False):\n",
    "        continue\n",
    "    # Load the pt file.\n",
    "    pt_file = glob.glob(os.path.join(wav_tensors_dir, file))\n",
    "    # Error check\n",
    "    if(pt_file == []):\n",
    "        continue\n",
    "    # Extract features and create labels in tensors.\n",
    "    features_tensor = torch.load(pt_file[0])\n",
    "    label_value = int(os.path.splitext(file)[0])\n",
    "    labels_tensor = torch.full((features_tensor.shape[0],), label_value, dtype=torch.long)\n",
    "    # Add these tensors to their respective lists.\n",
    "    all_features.append(features_tensor)\n",
    "    all_labels.append(labels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes for which data has been extracted: 106\n"
     ]
    }
   ],
   "source": [
    "# Print the number of classes for which data has been extracted.\n",
    "num_classes = len(all_labels)\n",
    "print(f\"Number of classes for which data has been extracted: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Error check for type of tensors.\n",
    "all_features[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of features tensor: torch.Size([19675, 320000])\n",
      "Shape of labels tensor: torch.Size([19675])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all the tensors into one tensor.\n",
    "features_tensor = torch.cat(all_features, dim=0)\n",
    "labels_tensor = torch.cat(all_labels, dim=0)\n",
    "\n",
    "# Encode the labels to make them suitable for training the model.\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels_tensor)\n",
    "# Create mapping dictionaries for the encoding.\n",
    "id_to_index = dict(zip(labels_tensor, encoded_labels))\n",
    "index_to_id = dict(zip(encoded_labels, labels_tensor))\n",
    "# Pickle dump these mappings for use later.\n",
    "with open('label_mappings.pkl', 'wb') as f:\n",
    "    pickle.dump({'id_to_index': id_to_index, 'index_to_id': index_to_id}, f)\n",
    "\n",
    "# Replace the labels tensor.\n",
    "labels_tensor = torch.from_numpy(encoded_labels)\n",
    "\n",
    "# Print out stats.\n",
    "print(f\"Shape of features tensor: {features_tensor.shape}\")\n",
    "print(f\"Shape of labels tensor: {labels_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we move on to splitting the data into training, validation, and test sets.\n",
    "# We will use a stratified split to ensure uniform distribution of classes.\n",
    "\n",
    "# Get labels as numpy array.\n",
    "labels_np = labels_tensor.numpy()\n",
    "# Use the labels np array to create indices array.\n",
    "indices = np.arange(len(labels_np))\n",
    "\n",
    "# First Split: train + val vs test - 80:20\n",
    "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "temp_idx, test_idx = next(sss1.split(indices, labels_np))\n",
    "\n",
    "# Get the temporary set.\n",
    "temp_indices = np.arange(len(temp_idx))\n",
    "temp_labels_np = labels_np[temp_idx]\n",
    "\n",
    "# Second Split: train vs val - 80:20\n",
    "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "train_temp_idx, val_temp_idx = next(sss2.split(temp_indices, temp_labels_np))\n",
    "\n",
    "# Convert to original indices.\n",
    "train_idx = temp_idx[train_temp_idx]\n",
    "val_idx = temp_idx[val_temp_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a full dataset combining the features and labels tensors.\n",
    "full_dataset = TensorDataset(features_tensor, labels_tensor)\n",
    "\n",
    "# Create subset datasets using the split indices.\n",
    "train_dataset = torch.utils.data.Subset(full_dataset, train_idx)\n",
    "val_dataset = torch.utils.data.Subset(full_dataset, val_idx)\n",
    "test_dataset = torch.utils.data.Subset(full_dataset, test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Wav2Vec as Feature Extractor & CNN Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"FOR MACBOOK LOCAL SETUP USERS ONLY \"\"\"\n",
    "use_mps = True\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"mps\") if use_mps and torch.backends.mps.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we start by setting up Wav2Vec to extract features from the audio data.\n",
    "# Define a custom dataset wrapper to incorporate wav2vec feature extraction.\n",
    "class Wav2VecFeatureDataset(Dataset):\n",
    "    def __init__(self, original_dataset, cache_features=True):\n",
    "        self.original_dataset = original_dataset\n",
    "        self.cache_features = cache_features\n",
    "        self.cached_features = {} if cache_features else None\n",
    "        \n",
    "        # Initialize wav2vec model and feature extractor\n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "        self.wav2vec_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "        self.wav2vec_model.to(device) # Shift to GPU if available.\n",
    "        self.wav2vec_model.eval()  # Set to evaluation mode since we're just extracting features\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.original_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get original data\n",
    "        audio, label = self.original_dataset[idx]\n",
    "        \n",
    "        # Check if features are already cached\n",
    "        if self.cache_features and idx in self.cached_features:\n",
    "            features = self.cached_features[idx]\n",
    "        else:\n",
    "            # Process audio through wav2vec\n",
    "            with torch.no_grad():\n",
    "                # Prepare inputs for wav2vec\n",
    "                inputs = self.feature_extractor(\n",
    "                    audio.cpu().numpy() if isinstance(audio, torch.Tensor) else audio, \n",
    "                    sampling_rate=16000, \n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                \n",
    "                # Shift to GPU if available, same device as model.\n",
    "                inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "                # Extract features\n",
    "                outputs = self.wav2vec_model(**inputs)\n",
    "                features = outputs.last_hidden_state.squeeze(0)  # Remove batch dimension\n",
    "            \n",
    "            # Cache if enabled\n",
    "            if self.cache_features:\n",
    "                self.cached_features[idx] = features\n",
    "        \n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders with wav2vec feature extraction\n",
    "def create_feature_dataloaders(train_dataset, val_dataset, test_dataset, batch_size):\n",
    "    # Wrap the original datasets with wav2vec feature extraction\n",
    "    train_feature_dataset = Wav2VecFeatureDataset(train_dataset)\n",
    "    val_feature_dataset = Wav2VecFeatureDataset(val_dataset)\n",
    "    test_feature_dataset = Wav2VecFeatureDataset(test_dataset)\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_feature_dataset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_feature_dataset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_feature_dataset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders using the feature extraction wrapper.\n",
    "batch_size = 32\n",
    "train_loader, val_loader, test_loader = create_feature_dataloaders(\n",
    "    train_dataset, val_dataset, test_dataset, batch_size)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ECE421",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
