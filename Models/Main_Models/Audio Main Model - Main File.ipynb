{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/archit436/Birds_Classifier/blob/main/Models/Main_Models/Main_Model_Audio_Main_File.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2Z5OlZ26H5S"
   },
   "source": [
    "0. Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2v-te8X34pLq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ECE421/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Start by importing the relevant libraries.\n",
    "# Copied from Archit's Lab 3 Submission and then some more.\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2FeatureExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Processing - WAV Tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Class ID</th>\n",
       "      <th>Images Count</th>\n",
       "      <th>XC Recordings Count</th>\n",
       "      <th>Species Name</th>\n",
       "      <th>XC Species Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>315</td>\n",
       "      <td>116</td>\n",
       "      <td>169</td>\n",
       "      <td>Gadwall (Breeding male)</td>\n",
       "      <td>Gadwall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>317</td>\n",
       "      <td>120</td>\n",
       "      <td>243</td>\n",
       "      <td>Mallard (Breeding male)</td>\n",
       "      <td>Mallard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>333</td>\n",
       "      <td>105</td>\n",
       "      <td>112</td>\n",
       "      <td>Common Goldeneye (Breeding male)</td>\n",
       "      <td>Common Goldeneye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>352</td>\n",
       "      <td>120</td>\n",
       "      <td>283</td>\n",
       "      <td>Black-crowned Night-Heron (Adult)</td>\n",
       "      <td>Black-crowned Night Heron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>366</td>\n",
       "      <td>101</td>\n",
       "      <td>127</td>\n",
       "      <td>Common Gallinule (Adult)</td>\n",
       "      <td>Common Gallinule</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index  Class ID  Images Count  XC Recordings Count  \\\n",
       "0      0       315           116                  169   \n",
       "1      1       317           120                  243   \n",
       "2      2       333           105                  112   \n",
       "3      3       352           120                  283   \n",
       "4      4       366           101                  127   \n",
       "\n",
       "                        Species Name            XC Species Name  \n",
       "0            Gadwall (Breeding male)                    Gadwall  \n",
       "1            Mallard (Breeding male)                    Mallard  \n",
       "2   Common Goldeneye (Breeding male)           Common Goldeneye  \n",
       "3  Black-crowned Night-Heron (Adult)  Black-crowned Night Heron  \n",
       "4           Common Gallinule (Adult)           Common Gallinule  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start by loading the birds dataframe for deets on birds to classify.\n",
    "birds_df = pd.read_csv('../../Data Processing/chosen_classes_80_data_stats.csv')\n",
    "birds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/7wj664s92gx_89p1xhcwbq800000gn/T/ipykernel_1964/834893210.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  features_tensor = torch.load(pt_file[0])\n"
     ]
    }
   ],
   "source": [
    "# Start by importing the tensors stored in the .pt files, one for each class.\n",
    "# Define the directory of the audio data.\n",
    "wav_tensors_dir = '../../Data/Xeno_Canto_WAV_Tensors'\n",
    "# Define a list to store the tensors and associated labels.\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "# Iterate through each bird, importing only the ones in birds_df.\n",
    "for file in os.listdir(wav_tensors_dir):\n",
    "    # Check if the file is a .pt file.\n",
    "    if(file.endswith('.pt') == False):\n",
    "        continue\n",
    "    # Load the pt file.\n",
    "    pt_file = glob.glob(os.path.join(wav_tensors_dir, file))\n",
    "    # Error check\n",
    "    if(pt_file == []):\n",
    "        continue\n",
    "    # Extract features and create labels in tensors.\n",
    "    features_tensor = torch.load(pt_file[0])\n",
    "    label_value = int(os.path.splitext(file)[0])\n",
    "    labels_tensor = torch.full((features_tensor.shape[0],), label_value, dtype=torch.long)\n",
    "    # Check if the label is in birds_df, skip if not.\n",
    "    if(label_value not in birds_df['Class ID'].values):\n",
    "        continue\n",
    "    # Add these tensors to their respective lists.\n",
    "    all_features.append(features_tensor)\n",
    "    all_labels.append(labels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes for which data has been extracted: 80\n"
     ]
    }
   ],
   "source": [
    "# Print the number of classes for which data has been extracted.\n",
    "num_classes = len(all_labels)\n",
    "print(f\"Number of classes for which data has been extracted: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Error check for type of tensors.\n",
    "all_features[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenation complete.\n",
      "Encoding complete.\n",
      "Mappings pickled.\n",
      "Shape of features tensor: torch.Size([17455, 320000])\n",
      "Shape of labels tensor: torch.Size([17455])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all the tensors into one tensor.\n",
    "features_tensor = torch.cat(all_features, dim=0)\n",
    "labels_tensor = torch.cat(all_labels, dim=0)\n",
    "\n",
    "print(\"Concatenation complete.\")\n",
    "\n",
    "# Encode the labels to make them suitable for training the model.\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels_tensor)\n",
    "# Create mapping dictionaries for the encoding.\n",
    "id_to_index = dict(zip(labels_tensor, encoded_labels))\n",
    "index_to_id = dict(zip(encoded_labels, labels_tensor))\n",
    "\n",
    "print(\"Encoding complete.\")\n",
    "\n",
    "# Pickle dump these mappings for use later.\n",
    "with open('label_mappings.pkl', 'wb') as f:\n",
    "    pickle.dump({'id_to_index': id_to_index, 'index_to_id': index_to_id}, f)\n",
    "\n",
    "print(\"Mappings pickled.\")\n",
    "\n",
    "# Replace the labels tensor.\n",
    "labels_tensor = torch.from_numpy(encoded_labels)\n",
    "\n",
    "# Print out stats.\n",
    "print(f\"Shape of features tensor: {features_tensor.shape}\")\n",
    "print(f\"Shape of labels tensor: {labels_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we move on to splitting the data into training, validation, and test sets.\n",
    "# We will use a stratified split to ensure uniform distribution of classes.\n",
    "\n",
    "# Get labels as numpy array.\n",
    "labels_np = labels_tensor.numpy()\n",
    "# Use the labels np array to create indices array.\n",
    "indices = np.arange(len(labels_np))\n",
    "\n",
    "# First Split: train + val vs test - 80:20\n",
    "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "temp_idx, test_idx = next(sss1.split(indices, labels_np))\n",
    "\n",
    "# Get the temporary set.\n",
    "temp_indices = np.arange(len(temp_idx))\n",
    "temp_labels_np = labels_np[temp_idx]\n",
    "\n",
    "# Second Split: train vs val - 80:20\n",
    "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "train_temp_idx, val_temp_idx = next(sss2.split(temp_indices, temp_labels_np))\n",
    "\n",
    "# Convert to original indices.\n",
    "train_idx = temp_idx[train_temp_idx]\n",
    "val_idx = temp_idx[val_temp_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a full dataset combining the features and labels tensors.\n",
    "full_dataset = TensorDataset(features_tensor, labels_tensor)\n",
    "\n",
    "# Create subset datasets using the split indices.\n",
    "train_dataset = torch.utils.data.Subset(full_dataset, train_idx)\n",
    "val_dataset = torch.utils.data.Subset(full_dataset, val_idx)\n",
    "test_dataset = torch.utils.data.Subset(full_dataset, test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Wav2Vec as Feature Extractor & CNN Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"FOR MACBOOK LOCAL SETUP USERS ONLY \"\"\"\n",
    "use_mps = True\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"mps\") if use_mps and torch.backends.mps.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we start by setting up Wav2Vec to extract features from the audio data.\n",
    "# Define a custom dataset wrapper to incorporate wav2vec feature extraction.\n",
    "class Wav2VecFeatureDataset(Dataset):\n",
    "    def __init__(self, original_dataset, cache_features=True):\n",
    "        self.original_dataset = original_dataset\n",
    "        self.cache_features = cache_features\n",
    "        self.cached_features = {} if cache_features else None\n",
    "        \n",
    "        # Initialize wav2vec model and feature extractor\n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "        self.wav2vec_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "        self.wav2vec_model.to(device) # Shift to GPU if available.\n",
    "        self.wav2vec_model.eval()  # Set to evaluation mode since we're just extracting features\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.original_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get original data\n",
    "        audio, label = self.original_dataset[idx]\n",
    "        \n",
    "        # Check if features are already cached\n",
    "        if self.cache_features and idx in self.cached_features:\n",
    "            features = self.cached_features[idx]\n",
    "        else:\n",
    "            # Process audio through wav2vec\n",
    "            with torch.no_grad():\n",
    "                # Prepare inputs for wav2vec\n",
    "                inputs = self.feature_extractor(\n",
    "                    audio.cpu().numpy() if isinstance(audio, torch.Tensor) else audio, \n",
    "                    sampling_rate=16000, \n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                \n",
    "                # Shift to GPU if available, same device as model.\n",
    "                inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "                # Extract features\n",
    "                outputs = self.wav2vec_model(**inputs)\n",
    "                features = outputs.last_hidden_state.squeeze(0)  # Remove batch dimension\n",
    "            \n",
    "            # Cache if enabled\n",
    "            if self.cache_features:\n",
    "                self.cached_features[idx] = features\n",
    "        \n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders with wav2vec feature extraction\n",
    "def create_feature_dataloaders(train_dataset, val_dataset, test_dataset, batch_size):\n",
    "    # Wrap the original datasets with wav2vec feature extraction\n",
    "    train_feature_dataset = Wav2VecFeatureDataset(train_dataset)\n",
    "    val_feature_dataset = Wav2VecFeatureDataset(val_dataset)\n",
    "    test_feature_dataset = Wav2VecFeatureDataset(test_dataset)\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_feature_dataset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_feature_dataset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_feature_dataset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a CNN model to train.\n",
    "class Wav2VecCNN(nn.Module):\n",
    "    def __init__(self, num_classes = num_classes, output_size=(32, 32)):\n",
    "        super(Wav2VecCNN, self).__init__()\n",
    "        self.name = \"Wav2VecCNN_AB_1\"\n",
    "\n",
    "        # Input shape is assumed to be [batch, 1, T, 768]\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Adaptive pooling to force output to a fixed size (output_size)\n",
    "        self.adapt_pool = nn.AdaptiveAvgPool2d(output_size)\n",
    "        \n",
    "        # Calculate fully connected layer input size:\n",
    "        # after two poolings, the number of channels is 32 and spatial dims become output_size.\n",
    "        fc_input_size = 32 * output_size[0] * output_size[1]\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(fc_input_size, 256)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x is expected to have shape [batch, T, 768]\n",
    "        # Add a channel dimension to make it [batch, 1, T, 768]\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        # Convolutional blocks\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        \n",
    "        # Use adaptive pooling to get a fixed-size output regardless of T\n",
    "        x = self.adapt_pool(x)\n",
    "        \n",
    "        # Flatten for fully connected layers\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create a name for each model on the basis of its hyperparameters.\n",
    "def get_model_name(name, batch_size, learning_rate, epoch):\n",
    "    \"\"\" Generate a name for the model consisting of all the hyperparameter values\n",
    "\n",
    "    Args:\n",
    "        config: Configuration object containing the hyperparameters\n",
    "    Returns:\n",
    "        path: A string with the hyperparameter name and value concatenated\n",
    "    \"\"\"\n",
    "    path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,\n",
    "                                                   batch_size,\n",
    "                                                   learning_rate,\n",
    "                                                   epoch)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(model, batch_size, train_loader, val_loader, learning_rate=0.001, num_epochs=20):\n",
    "    print(f\"Now training model with spec: {model.name}\")\n",
    "\n",
    "    # # Fixed PyTorch random seed for reproducibility\n",
    "    # torch.manual_seed(1000)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Define a scheduler to control the learning rate.\n",
    "    # It will reduce the LR when the validation loss has stopped improving.\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "    print(\"Loss Function, Optimizer, and Scheduler set up.\")\n",
    "\n",
    "    # Arrays to store accuracy metrics\n",
    "    train_acc = np.zeros(num_epochs)\n",
    "    val_acc = np.zeros(num_epochs)\n",
    "\n",
    "    # Create an output folder for performance files\n",
    "    output_folder = \"Audio_Model_Performance\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Early Stopping to prevent overfitting\n",
    "    best_val_acc = 0.0\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(\"Training Started.\")\n",
    "\n",
    "    # Iterate for number of epochs.\n",
    "    for epoch in range(num_epochs):\n",
    "        # --- Training Phase ---\n",
    "        model.train()\n",
    "        # Forwards and backwards pass for each batch\n",
    "        for _, data in enumerate(train_loader, 0):\n",
    "            recordings, labels = data\n",
    "            recordings = recordings.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(recordings)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # clip gradients to prevent exploding gradients (LSTM)\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f\"Finished adjusting parameters for epoch {epoch + 1}\")\n",
    "\n",
    "        # --- Evaluation Phase ---\n",
    "        model.eval()\n",
    "        correct_t, total_t = 0, 0\n",
    "        correct_v, total_v = 0, 0\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        with torch.no_grad():\n",
    "            # Forward pass for each batch\n",
    "            for recordings, labels in train_loader:\n",
    "                recordings = recordings.to(device)\n",
    "                labels = labels.to(device)\n",
    "                output = model(recordings)\n",
    "                pred = output.max(1, keepdim=True)[1]\n",
    "                correct_t += pred.eq(labels.view_as(pred)).sum().item()\n",
    "                total_t += recordings.shape[0]\n",
    "        # Calculate training accuracy        \n",
    "        train_acc[epoch] = correct_t / total_t\n",
    "\n",
    "        # Calculate validation accuracy and loss\n",
    "        with torch.no_grad():\n",
    "            # Forward pass for each batch\n",
    "            for recordings, labels in val_loader:\n",
    "                recordings = recordings.to(device)\n",
    "                labels = labels.to(device)\n",
    "                output = model(recordings)\n",
    "                loss_val = criterion(output, labels)\n",
    "                val_loss += loss_val.item()  # accumulate loss\n",
    "                pred = output.max(1, keepdim=True)[1]\n",
    "                correct_v += pred.eq(labels.view_as(pred)).sum().item()\n",
    "                total_v += recordings.shape[0]\n",
    "        # Calculate validation accuracy and loss.        \n",
    "        val_acc[epoch] = correct_v / total_v\n",
    "        val_loss /= len(val_loader)  # average validation loss\n",
    "        \n",
    "        # Update the learning rate based on validation loss.\n",
    "        # scheduler.step(val_loss)\n",
    "        \n",
    "        # Print epoch results including validation loss.\n",
    "        print(f\"Epoch {epoch + 1}: Train acc: {train_acc[epoch]:.4f}, \"\n",
    "              f\"Validation acc: {val_acc[epoch]:.4f}\")\n",
    "        \n",
    "        # Early Stopping\n",
    "        # Case 1: Validation accuracy has increased.\n",
    "        if val_acc[epoch] > best_val_acc:\n",
    "            # Reset the parameters.\n",
    "            best_val_acc = val_acc[epoch]\n",
    "            patience_counter = 0\n",
    "        # Case 2: Validation accuracy has not increased.\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        # Check if patience has been exceeded.\n",
    "        if patience_counter > patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    print('Finished Training')\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    model_filename = get_model_name(model.name, batch_size, learning_rate, num_epochs - 1)\n",
    "    model_path = os.path.join(output_folder, model_filename)\n",
    "    train_acc_path = f\"{model_path}_train_acc.csv\"\n",
    "    val_acc_path = f\"{model_path}_val_acc.csv\"\n",
    "    np.savetxt(train_acc_path, train_acc[:epoch + 1])\n",
    "    np.savetxt(val_acc_path, val_acc[:epoch + 1])\n",
    "    \n",
    "    return train_acc[:epoch + 1], val_acc[:epoch + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ECE421/lib/python3.10/site-packages/transformers/configuration_utils.py:315: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders using the feature extraction wrapper.\n",
    "batch_size = 32\n",
    "train_loader, val_loader, test_loader = create_feature_dataloaders(\n",
    "    train_dataset, val_dataset, test_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now training model with spec: Wav2VecCNN_AB_1\n",
      "Loss Function, Optimizer, and Scheduler set up.\n",
      "Training Started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ECE421/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "python(2276) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(2277) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Create an instance of the model.\n",
    "model = Wav2VecCNN()\n",
    "\n",
    "# Move the model to GPU if available.\n",
    "model.to(device)\n",
    "\n",
    "# Train the model\n",
    "train_acc, val_acc = train_net(model, batch_size, train_loader, val_loader,\n",
    "                                   learning_rate = 0.001, num_epochs = 30)\n",
    "n = len(train_acc)\n",
    "plt.title(\"Training Curve\")\n",
    "plt.plot(range(1,n+1), train_acc, label=\"Train\")\n",
    "plt.plot(range(1,n+1), val_acc, label=\"Validation\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ECE421",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
